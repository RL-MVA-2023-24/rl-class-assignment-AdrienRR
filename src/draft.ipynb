{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "from functools import partial\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from env_hiv import HIVPatient\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b4799ddb90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ")\n",
    "\n",
    "# Constants\n",
    "NB_EPISODES = 500\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 20000\n",
    "EPS_LAG = 500\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "BATCH_SIZE = 1024\n",
    "TARGET_UPDATE_PERIOD = 1000\n",
    "\n",
    "# Seed for reproducibility\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn_agent:\n",
    "    def __init__(self, config):\n",
    "        self.device = config['device']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size, self.device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = torch.nn.Sequential(nn.Linear(env.observation_space.shape[0], config['nb_neurons']),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], env.action_space.n)).to(self.device) \n",
    "        self.target_model = deepcopy(self.model).to(self.device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            Q = self.model(torch.Tensor(state).unsqueeze(0).to(self.device))\n",
    "        return torch.argmax(Q).item()\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "\n",
    "    def save(self, path):\n",
    "        # Save the model on the CPU\n",
    "        torch.save(self.model.to('cpu').state_dict(), path)\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        # When we load the model we just use it for evaluation so we make sure the actions are fuly driven by the model\n",
    "        self.epsilon_max = 0\n",
    "        self.epsilon_min = 0\n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "        if use_random:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = self.greedy_action(observation)\n",
    "        return action\n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "            action = self.act(state, np.random.rand() < epsilon)\n",
    "\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            \n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "\n",
    "                # Save the model if it is the best so far\n",
    "                if len(episode_return) == 0 or episode_cum_reward >= max(episode_return):\n",
    "                    print(\"New best model saved\")\n",
    "                    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    self.save(f\"best_model_{now}.pt\")\n",
    "\n",
    "                episode_return.append(episode_cum_reward)\n",
    "\n",
    "                episode_cum_reward = 0\n",
    "                if episode % 10 == 0:\n",
    "                    plt.plot(episode_return)\n",
    "                    plt.xlabel('Episode')\n",
    "                    plt.ylabel('Return')\n",
    "                    plt.title('Return per Episode')\n",
    "                    plt.show()\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn_agent_with_memory:\n",
    "    def __init__(self, config):\n",
    "        self.device = config['device']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, self.device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.memory_length = config['memory_length'] if 'memory_length' in config.keys() else 10\n",
    "        self.memory = deque([-1 for _ in range(env.observation_space.shape[0] + self.memory_length)], maxlen=self.memory_length)\n",
    "        self.model = torch.nn.Sequential(nn.Linear(env.observation_space.shape[0] + self.memory_length, config['nb_neurons']),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], config['nb_neurons']),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(config['nb_neurons'], env.action_space.n)).to(self.device) \n",
    "        self.target_model = deepcopy(self.model).to(self.device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "    \n",
    "    def normalize_state(self, state):\n",
    "        normalization_factors = torch.Tensor([1e5, 1e4, 1e1, 1e4, 1e1, 1e1] + [1] * self.memory_length).to(self.device)\n",
    "        state = state / normalization_factors   \n",
    "        return state\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state_and_memory = torch.Tensor(np.concatenate((state, self.memory))).unsqueeze(0).to(self.device)\n",
    "            state_and_memory = self.normalize_state(state_and_memory)\n",
    "            Q = self.model(state_and_memory)\n",
    "        return torch.argmax(Q).item()\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            X, A, R, Y, D = self.replay_buffer.sample(self.batch_size)\n",
    "            X = self.normalize_state(X)\n",
    "            Y = self.normalize_state(Y)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "\n",
    "    def save(self, path):\n",
    "        # Save the model on the CPU\n",
    "        torch.save(self.model.to('cpu').state_dict(), path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        # When we load the model we just use it for evaluation so we make sure the actions are fuly driven by the model\n",
    "        self.epsilon_max = 0\n",
    "        self.epsilon_min = 0\n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "        if use_random:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = self.greedy_action(observation)\n",
    "        self.memory.append(action)\n",
    "        return action\n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "            action = self.act(state, np.random.rand() < epsilon)\n",
    "\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.replay_buffer.append(np.concatenate((state, self.memory)), action, reward, np.concatenate((next_state, self.memory)), done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            \n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.replay_buffer)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "\n",
    "                # Save the model if it is the best so far\n",
    "                if len(episode_return) == 0 or episode_cum_reward >= max(episode_return):\n",
    "                    print(\"New best model saved\")\n",
    "                    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                    self.save(f\"best_model_{now}.pt\")\n",
    "\n",
    "                episode_return.append(episode_cum_reward)\n",
    "\n",
    "                episode_cum_reward = 0\n",
    "                if episode % 10 == 0:\n",
    "                    plt.plot(episode_return)\n",
    "                    plt.xlabel('Episode')\n",
    "                    plt.ylabel('Return')\n",
    "                    plt.title('Return per Episode')\n",
    "                    plt.show()\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode   1, epsilon   1.00, batch size   200, episode return 8411206.0\n",
      "New best model saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m agent \u001b[39m=\u001b[39m dqn_agent(config)\n\u001b[0;32m     30\u001b[0m \u001b[39m# agent = dqn_agent_with_memory(config)\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m scores \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(env, \u001b[39m500\u001b[39;49m)\n\u001b[0;32m     32\u001b[0m plt\u001b[39m.\u001b[39mplot(scores)\n",
      "Cell \u001b[1;32mIn[16], line 99\u001b[0m, in \u001b[0;36mdqn_agent.train\u001b[1;34m(self, env, max_episode)\u001b[0m\n\u001b[0;32m     97\u001b[0m     tau \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_target_tau\n\u001b[0;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m model_state_dict:\n\u001b[1;32m---> 99\u001b[0m         target_state_dict[key] \u001b[39m=\u001b[39m tau\u001b[39m*\u001b[39;49mmodel_state_dict[key] \u001b[39m+\u001b[39;49m (\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49mtau)\u001b[39m*\u001b[39;49mtarget_state_dict[key]\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model\u001b[39m.\u001b[39mload_state_dict(target_state_dict)\n\u001b[0;32m    102\u001b[0m \u001b[39m# next transition\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Declare network\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': n_action,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma':0.99,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.05,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 15000,#10000,\n",
    "          'epsilon_delay_decay': 1000,#300,\n",
    "          'batch_size': 1024,\n",
    "          'gradient_steps': 5,\n",
    "          'update_target_strategy': 'ema',#'replace', # or 'ema'\n",
    "          'update_target_freq': 200,\n",
    "          'update_target_tau': 0.001,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'nb_neurons': 256,\n",
    "          'device': device,\n",
    "          'memory_length':5}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config)\n",
    "# agent = dqn_agent_with_memory(config)\n",
    "scores = agent.train(env, 500)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = \"best_model_20211018-191153.pt\"\n",
    "agent = dqn_agent_with_memory(config)\n",
    "agent.load(path_to_trained_model)\n",
    "\n",
    "def evaluate_agent(agent: dqn_agent_with_memory, env: gym.Env, nb_episode: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an agent in a given environment.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): The agent to evaluate.\n",
    "        env (gym.Env): The environment to evaluate the agent in.\n",
    "        nb_episode (int): The number of episode to evaluate the agent.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean reward of the agent over the episodes.\n",
    "    \"\"\"\n",
    "    rewards: list[float] = []\n",
    "    for _ in range(nb_episode):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        while not done and not truncated:\n",
    "            action = agent.act(obs)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        rewards.append(episode_reward)\n",
    "    return mean(rewards)\n",
    "\n",
    "\n",
    "evaluate_HIV = partial(\n",
    "    evaluate_agent, env=TimeLimit(HIVPatient(), max_episode_steps=200)\n",
    ")\n",
    "\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
